{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bigram nn approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Loss: 3.751142740249634\n",
      "Iteration 1 Loss: 3.5050597190856934\n",
      "Iteration 2 Loss: 3.3318803310394287\n",
      "Iteration 3 Loss: 3.203662157058716\n",
      "Iteration 4 Loss: 3.1074867248535156\n",
      "Iteration 5 Loss: 3.032045602798462\n",
      "Iteration 6 Loss: 2.9698197841644287\n",
      "Iteration 7 Loss: 2.917283296585083\n",
      "Iteration 8 Loss: 2.8726439476013184\n",
      "Iteration 9 Loss: 2.83463978767395\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "W = torch.randn((27, 27), requires_grad=True)\n",
    "\n",
    "for i in range(10):\n",
    "    # flatten the two arrays in each row into a single array\n",
    "    xenc= F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(f\"Iteration {i} Loss: {loss.item()}\")\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -30.0 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexzdaleglkurailezitxhn.\n",
      "vinimjttain.\n",
      "lgfykzka.\n",
      "ar.\n",
      "swaivpubptvhrigotzi.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes = 27).float()\n",
    "        logits = xenc @ W # predict log counts\n",
    "        # softmax (next two lines)\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        probs = counts / counts.sum(1, keepdim=True) # probabiltiies for the next character\n",
    "\n",
    "        # sample from a distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trigram nn approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append([ix1, ix2])\n",
    "        ys.append(ix3)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Loss: 4.1346611976623535\n",
      "Iteration 1 Loss: 3.586719512939453\n",
      "Iteration 2 Loss: 3.2827024459838867\n",
      "Iteration 3 Loss: 3.1056246757507324\n",
      "Iteration 4 Loss: 2.980825424194336\n",
      "Iteration 5 Loss: 2.8855695724487305\n",
      "Iteration 6 Loss: 2.8106770515441895\n",
      "Iteration 7 Loss: 2.7505600452423096\n",
      "Iteration 8 Loss: 2.7015597820281982\n",
      "Iteration 9 Loss: 2.6611087322235107\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "W = torch.randn((54, 27), requires_grad=True)\n",
    "\n",
    "for i in range(10):\n",
    "    # flatten the two arrays in each row into a single array\n",
    "    xenc= F.one_hot(xs, num_classes=27).float().view(-1, 54)\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    loss = -probs[torch.arange(num//2), ys].log().mean()\n",
    "    print(f\"Iteration {i} Loss: {loss.item()}\")\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -30.0 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dexzmriogjpurkylqzvqxh.\n",
      "ellzimjttain.\n",
      "augak.\n",
      "man.\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    context = [0, 0]\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([context]), num_classes = 27).float().view(1, -1)\n",
    "        logits = xenc @ W # predict log counts\n",
    "        # softmax (next two lines)\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        probs = counts / counts.sum(1, keepdim=True) # probabiltiies for the next character\n",
    "\n",
    "        # sample from a distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        context = context[1:] + [ix] # cause you're using two letters to predict, not one.\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIGRAM AND TRIGRAM CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Test Loss: 2.864698648452759\n",
      "X_Dev Loss: 2.864078998565674\n",
      "cexbmaloglkurkicczktyhwbvmzimjttainrlkfukzka.\n",
      "da.\n",
      "sfcxvpubjtbhrmiotzx.\n",
      "mczieqckvujkwptedogkkjemvmmsidguenkavgynywftbspmhwcivgbvtahlvsu.\n",
      "dsdxxblnwglhpyaw.\n",
      "iswn.\n",
      "wrpfdwipkezkm.\n",
      "deru.\n",
      "firmt.\n",
      "gbikajbquabsvath.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Bigram:\n",
    "    def __init__(self, data, num_inputs, num_outputs, training_epochs):\n",
    "        self.data = data\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.training_epochs = training_epochs\n",
    "    \n",
    "    def generate_data(self):\n",
    "        chars = sorted(list(set(''.join(words))))\n",
    "        self.stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "        self.stoi['.'] = 0\n",
    "        self.itos = {i:s for s,i in self.stoi.items()}\n",
    "\n",
    "        xs, ys = [], []\n",
    "        for w in words:\n",
    "            chs = ['.'] + list(w) + ['.']\n",
    "            for ch1, ch2 in zip(chs, chs[1:]):\n",
    "                ix1 = self.stoi[ch1]\n",
    "                ix2 = self.stoi[ch2]\n",
    "                xs.append(ix1)\n",
    "                ys.append(ix2)\n",
    "                \n",
    "        xs = torch.tensor(xs)\n",
    "        ys = torch.tensor(ys)\n",
    "        self.num = xs.nelement()\n",
    "        return xs, ys\n",
    "    \n",
    "    def split_data(self, xs, ys, train_p, test_p):\n",
    "        # missing percentage goes to validation set\n",
    "        xs = np.array(xs)\n",
    "        ys = np.array(ys)\n",
    "        X, y = shuffle(xs, ys)\n",
    "        num_examples = len(X)\n",
    "\n",
    "        X_train = torch.tensor(X[0:int(train_p * num_examples)])\n",
    "        y_train = torch.tensor(y[0:int(train_p * num_examples)])\n",
    "\n",
    "        X_test = torch.tensor(X[int(train_p * num_examples): int((train_p + test_p) * num_examples)])\n",
    "        y_test = torch.tensor(y[int(train_p * num_examples): int((train_p + test_p) * num_examples)])\n",
    "\n",
    "        X_dev = torch.tensor(X[int((train_p + test_p) * num_examples): num_examples])\n",
    "        y_dev = torch.tensor(y[int((train_p + test_p) * num_examples): num_examples])\n",
    "\n",
    "        return X_train, y_train, X_test, y_test, X_dev, y_dev\n",
    "    \n",
    "    def fit(self, X, y, print_loss, reg_value):\n",
    "        W = torch.randn((self.num_outputs, self.num_inputs), requires_grad=True)\n",
    "\n",
    "        for i in range(self.training_epochs):\n",
    "            # xenc = F.one_hot(X, num_classes=self.num_inputs).float()\n",
    "            # logits = xenc @ W\n",
    "            logits = W[X, :]\n",
    "            counts = logits.exp()\n",
    "            probs = counts / counts.sum(1, keepdim=True)\n",
    "            loss = -probs[torch.arange(len(X)), y].log().mean() + reg_value * (W**2).mean()\n",
    "            if(print_loss):\n",
    "                print(f\"Iteration {i} Loss: {loss.item()}\")\n",
    "\n",
    "            W.grad = None\n",
    "            loss.backward()\n",
    "            W.data += -30.0 * W.grad\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def evaluate_model(self, W, X, y, is_test):\n",
    "        # xenc = F.one_hot(X, num_classes=self.num_outputs).float()\n",
    "        # logits = xenc @ W\n",
    "        logits = W[X, :]\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdim=True)\n",
    "        loss = -probs[torch.arange(len(X)), y].log().mean()\n",
    "        if is_test:\n",
    "            print(f\"X_Test Loss: {loss}\")\n",
    "        else:\n",
    "            print(f\"X_Dev Loss: {loss}\")\n",
    "    \n",
    "    def generate_samples(self, num_words, W):\n",
    "        g = torch.Generator().manual_seed(2147483647)\n",
    "        for i in range(num_words):\n",
    "            out = []\n",
    "            ix = 0\n",
    "            while True:\n",
    "                # xenc = F.one_hot(torch.tensor([ix]), num_classes = 27).float()\n",
    "                # logits = xenc @ W # predict log counts\n",
    "                \n",
    "                logits = W[ix, :]\n",
    "                counts = logits.exp() # counts, equivalent to N\n",
    "                probs = counts / counts.sum() # probabiltiies for the next character\n",
    "\n",
    "                # sample from a distribution\n",
    "                ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "                out.append(self.itos[ix])\n",
    "                if ix == 0:\n",
    "                    break\n",
    "            print(''.join(out)) \n",
    "\n",
    "my_bigram = Bigram(words, 27, 27, 10)\n",
    "xs, ys = my_bigram.generate_data()\n",
    "X_train, y_train, X_test, y_test, X_dev, y_dev = my_bigram.split_data(xs, ys, 0.8, 0.1)\n",
    "W = my_bigram.fit(X_train, y_train, False, 2.0)\n",
    "\n",
    "my_bigram.evaluate_model(W, X_test, y_test, True)\n",
    "my_bigram.evaluate_model(W, X_dev, y_dev, False)\n",
    "my_bigram.generate_samples(10, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Test Loss: 2.949021816253662\n",
      "X_Dev Loss: 2.9497745037078857\n",
      "dexbm.\n",
      "ioglkurxicazkwyhnevlzimjtnainrlkfuk.\n",
      "kataa.\n",
      "rnaxypubjtbhr.\n",
      "iotai.\n",
      "iczixqckxugnwptedagek.\n",
      "emvmmsadlu.\n",
      "nkavnynyhftbsp.\n",
      "hwnivebvtahlvsu.\n",
      "nsdrx.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFor the same learning rate and number of epochs, trigram has a \\nlower loss than bigram. \\n'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Trigram:\n",
    "    def __init__(self, data, num_inputs, num_outputs, training_epochs):\n",
    "        self.data = data\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.training_epochs = training_epochs\n",
    "    \n",
    "    def generate_data(self):\n",
    "        chars = sorted(list(set(''.join(words))))\n",
    "        self.stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "        self.stoi['.'] = 0\n",
    "        self.itos = {i:s for s,i in self.stoi.items()}\n",
    "\n",
    "        xs, ys = [], []\n",
    "        for w in words:\n",
    "            chs = ['.'] + list(w) + ['.']\n",
    "            for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "                ix1 = self.stoi[ch1]\n",
    "                ix2 = self.stoi[ch2]\n",
    "                ix3 = self.stoi[ch3]\n",
    "                xs.append([ix1, ix2])\n",
    "                ys.append(ix3)\n",
    "                \n",
    "        xs = torch.tensor(xs)\n",
    "        ys = torch.tensor(ys)\n",
    "        self.num = xs.nelement()\n",
    "        return xs, ys\n",
    "    \n",
    "    def split_data(self, xs, ys, train_p, test_p):\n",
    "        # missing percentage goes to validation set\n",
    "        xs = np.array(xs)\n",
    "        ys = np.array(ys)\n",
    "        X, y = shuffle(xs, ys)\n",
    "        num_examples = len(X)\n",
    "\n",
    "        X_train = torch.tensor(X[0:int(train_p * num_examples)])\n",
    "        y_train = torch.tensor(y[0:int(train_p * num_examples)])\n",
    "\n",
    "        X_test = torch.tensor(X[int(train_p * num_examples): int((train_p + test_p) * num_examples)])\n",
    "        y_test = torch.tensor(y[int(train_p * num_examples): int((train_p + test_p) * num_examples)])\n",
    "\n",
    "        X_dev = torch.tensor(X[int((train_p + test_p) * num_examples): num_examples])\n",
    "        y_dev = torch.tensor(y[int((train_p + test_p) * num_examples): num_examples])\n",
    "\n",
    "        return X_train, y_train, X_test, y_test, X_dev, y_dev\n",
    "    \n",
    "    def fit(self, X, y, print_loss, reg_value):\n",
    "        W = torch.randn((self.num_inputs, self.num_outputs), requires_grad=True)\n",
    "\n",
    "        for i in range(self.training_epochs):\n",
    "            # flatten the two arrays in each row into a single array\n",
    "            # xenc = F.one_hot(X, num_classes=self.num_outputs).float().view(-1, self.num_inputs)\n",
    "            # logits = xenc @ W\n",
    "\n",
    "            logits = W[X.flatten(), :]\n",
    "            counts = logits.exp()\n",
    "            probs = counts / counts.sum(1, keepdim=True)\n",
    "            loss = -probs[torch.arange(len(X)), y].log().mean() + reg_value * (W**2).mean()\n",
    "            if(print_loss):\n",
    "                print(f\"Iteration {i} Loss: {loss.item()}\")\n",
    "\n",
    "            W.grad = None\n",
    "            loss.backward()\n",
    "            W.data += -30.0 * W.grad\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def evaluate_model(self, W, X, y, is_test):\n",
    "        # xenc = F.one_hot(X, num_classes=self.num_outputs).float().view(-1, self.num_inputs)\n",
    "        # logits = xenc @ W\n",
    "\n",
    "        logits = W[X.flatten(), :]\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdim=True)\n",
    "        loss = -probs[torch.arange(len(X)), y].log().mean()\n",
    "        if is_test:\n",
    "            print(f\"X_Test Loss: {loss}\")\n",
    "        else:\n",
    "            print(f\"X_Dev Loss: {loss}\")\n",
    "    \n",
    "    def generate_samples(self, num_words, W):\n",
    "        g = torch.Generator().manual_seed(2147483647)\n",
    "        for i in range(num_words):\n",
    "            out = []\n",
    "            context = [0, 0]\n",
    "            while True:\n",
    "                xenc = F.one_hot(torch.tensor([context]), num_classes = 27).float().view(-1, 1)\n",
    "                logits = xenc.T @ W # predict log counts\n",
    "                # softmax (next two lines)\n",
    "\n",
    "                logits = W[context, :][0]\n",
    "                counts = logits.exp() # counts, equivalent to N\n",
    "                probs = counts / counts.sum() # probabiltiies for the next character\n",
    "                # sample from a distribution\n",
    "                ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "                context = context[1:] + [ix]\n",
    "                out.append(self.itos[ix])\n",
    "                if ix == 0:\n",
    "                    break\n",
    "            print(''.join(out)) \n",
    "\n",
    "my_trigram = Trigram(words, 54, 27, 10)\n",
    "xs, ys = my_trigram.generate_data()\n",
    "X_train, y_train, X_test, y_test, X_dev, y_dev = my_trigram.split_data(xs, ys, 0.8, 0.1)\n",
    "W = my_trigram.fit(X_train, y_train, False, 2.0)\n",
    "\n",
    "my_trigram.evaluate_model(W, X_test, y_test, True)\n",
    "my_trigram.evaluate_model(W, X_dev, y_dev, False)\n",
    "my_trigram.generate_samples(10, W)\n",
    "\n",
    "\"\"\"\n",
    "For the same learning rate and number of epochs, trigram has a \n",
    "lower loss than bigram. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
